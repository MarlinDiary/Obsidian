#最大熵 #机器学习 #逻辑回归 #特征工程

> 本文探讨为什么在最大熵推导中会出现实值特征函数 \(f_i(x,y) = x_i \cdot I[y=1]\)，以及如何通过这种方式自然地推导出 [[逻辑回归]] 的形式。这个推导过程避免了循环论证，不依赖于"事先知道答案是 Logistic 回归"的假设。

## 1. 最大熵的核心：用"期望约束"表达统计量

最大熵模型的本质在于：
1. 列出必须在模型中正确反映的统计性质（约束或特征期望）
2. 在满足这些约束的所有候选分布中，选取熵最大的那个

关键点：**"特征函数"决定了你要对哪些'统计量'施加期望约束**。它不必限于 0/1 事件，可以是任何实值函数。

### 1.1 为什么有时只用 0/1 指示？

在 [[自然语言处理]] 或离散特征场景中：
- 通常只关注"某个事件是否发生"在特定标签下的频率
- 例如："单词 free 是否出现在垃圾邮件中"
- 这类离散计数场景，使用 0/1 指示函数最为直接

### 1.2 实数型特征的必要性

当数据 \(\mathbf{x}=(x_1,\dots,x_d)\) 包含数值特征时：
- 特征与标签之间存在统计规律，如"当标签=1时，\(x_1\) 的平均值"
- 需要模型正确再现这些统计量
- 单纯的 0/1 指示函数无法直接反映实值的统计特性

## 2. 实值特征函数与线性相关性

对于二分类问题（\(y\in\{0,1\}\)），我们关注：
> "当 \(y=1\) 时，\(x_i\) 的平均值应与训练数据一致"

这可以通过期望约束表达：

\[
\sum_{x,y} \tilde{P}(x)\,P(y\mid x)\,f_i(x,y)
\;=\;
\sum_{(x,y)\in \text{训练集}} \tilde{P}(x,y)\,f_i(x,y)
\]

定义特征函数：
\[
f_i(x,y)=
\begin{cases}
  x_i, & y=1,\\
  0,   & y=0,
\end{cases}
\]

这确保了模型能够匹配训练集中 \(y=1\) 条件下 \(x_i\) 的平均值。

## 3. 从最大熵到 Logistic 形式

最大熵原理表明：
- 满足线性期望约束且熵最大的分布必属于指数家族
- 对二分类的条件概率，可推导出：

\[
P(y=1\mid x)
=
\frac{\exp(\lambda_0 + \lambda_1 x_1 + \cdots + \lambda_d x_d)}{
  1 + \exp(\lambda_0 + \lambda_1 x_1 + \cdots + \lambda_d x_d)
}
\]

这里的 Logistic 形式是推导的结果，而非预设。

## 4. 与标准 0/1 指示函数的关系

两种特征函数各有用途：
- 0/1 指示函数：表达频率约束
- \(x_i\cdot I[y=1]\)：表达条件均值约束
- 都是特征函数，只是一个离散，一个实值

## 5. 要点总结

1. **目的**：在模型中准确再现条件均值等实值统计量
2. **方法**：使用 \(f_i(x,y)=x_i\cdot I[y=1]\) 表达约束
3. **结果**：最大熵推导自然得到 Logistic 形式
4. **非循环**：Logistic 回归是最大熵在特定约束下的特解

相关笔记：
- [[最大熵原理]]
- [[特征工程]]
- [[指数族分布]] 