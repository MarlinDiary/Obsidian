标准差（Standard Deviation）是一个在[[统计学]]中用于衡量数据分散程度的重要指标。它描述了数据点相对于平均值的离散程度，与[[方差]]密切相关。

## 数学定义

标准差的计算公式为：

$$
\sigma = \sqrt{\frac{\sum_{i=1}^{n} (x_i - \mu)^2}{n}}
$$

其中：
- $\sigma$ 是标准差
- $x_i$ 是每个数据点
- $\mu$ 是[[均值|算术平均值]]
- $n$ 是数据点的数量

## 计算步骤

1. 计算数据集的算术平均值
2. 计算每个数据点与平均值的差
3. 将差值平方
4. 求平方值的平均数
5. 对结果开平方根

## 应用场景

### 金融领域
- 衡量投资[[风险]]
- 评估资产价格波动性
- 构建投资[[投资组合|组合]]

### 质量控制
- 产品质量检测
- 生产过程监控
- [[异常检测]]

### 科学研究
- 实验数据分析
- 测量误差评估
- [[数据分析]]可靠性验证

### 教育评估
- 考试成绩分析
- 学生表现评估
- 教学效果衡量

## 特点

- 标准差始终为非负数
- 标准差的单位与原始数据相同
- 标准差越大，数据分散程度越高
- 标准差为零表示所有数据值都相同

## 相关概念

- [[正态分布]]中的标准差具有特殊意义
- 在[[机器学习]]中常用于数据标准化
- 与[[中位数]]、[[四分位数]]等统计量共同描述数据分布

#统计学 #数据分析 #数学 