链式法则（Chain Rule）是[[微积分]]中的一个基本法则，在[[机器学习]]和[[深度学习]]中的[[反向传播]]算法中起着核心作用。它描述了复合函数的导数计算方法。

## 基本概念

### 单变量情况
对于复合函数 $f(g(x))$，其导数为：

$$
\frac{d}{dx}[f(g(x))] = f'(g(x)) \cdot g'(x)
$$

通常写作：
$$
\frac{dy}{dx} = \frac{dy}{du} \cdot \frac{du}{dx}
$$

### 多变量情况
对于多变量函数，偏导数的链式法则：

$$
\frac{\partial f}{\partial x} = \sum_i \frac{\partial f}{\partial u_i} \cdot \frac{\partial u_i}{\partial x}
$$

## 直观理解

1. **变化率的传递**
   - 链式法则描述了变化如何通过函数链传递
   - 每一步的变化率相乘得到最终的变化率

2. **类比理解**
   - 如果把函数看作变换，链式法则描述了复合变换的整体效果
   - 类似于汇率转换：美元→欧元→人民币，总转换率是各步骤转换率的乘积

## 在机器学习中的应用

1. **[[反向传播]]算法**
   - 计算损失函数对各层参数的梯度
   - 梯度从输出层向输入层逐层传播

2. **[[特征缩放与梯度计算]]**
   - 计算标准化数据的梯度转换
   - $\frac{\partial L}{\partial x} = \frac{\partial L}{\partial z} \cdot \frac{\partial z}{\partial x}$

3. **[[神经网络]]训练**
   - 计算复杂网络结构中的梯度
   - 优化网络参数

## 实际例子

### 神经网络中的应用
假设有一个简单的神经网络：
```
输入 x → 权重 w → 激活函数 σ → 损失函数 L
```

梯度计算过程：
$$
\frac{\partial L}{\partial w} = \frac{\partial L}{\partial \sigma} \cdot \frac{\partial \sigma}{\partial (wx)} \cdot \frac{\partial (wx)}{\partial w}
$$

### 标准化数据的梯度
对于Z-score标准化：
$$
\frac{\partial L}{\partial x} = \frac{\partial L}{\partial z} \cdot \frac{1}{\sigma}
$$

## 注意事项

1. **计算顺序**
   - 从外层函数开始，逐层向内计算
   - 保持正确的求导顺序很重要

2. **数值稳定性**
   - 链式求导可能导致梯度消失或爆炸
   - 需要合适的[[激活函数]]和[[参数初始化]]

3. **自动微分**
   - 现代深度学习框架（[[PyTorch]]、[[TensorFlow]]）自动处理链式法则
   - 大大简化了梯度计算的复杂性

#数学 #机器学习 #深度学习 #优化 