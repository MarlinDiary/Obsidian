梯度下降（Gradient Descent）是[[机器学习]]中最基本的优化算法之一，用于找到函数的局部最小值。它的工作原理基于[[微积分]]中导数的几何意义。

## 导数的几何意义

1. **导数表示斜率**
   - 导数 $f'(x)$ 在几何上表示函数在该点的切线斜率
   - 正导数表示函数在增加
   - 负导数表示函数在减少
   - 导数为零表示函数在该点水平

2. **导数指向上升最快的方向**
   - 梯度 $\nabla f$ 指向函数值上升最快的方向
   - 负梯度 $-\nabla f$ 指向函数值下降最快的方向

## 为什么减去导数有效

1. **基本原理**
   - 参数更新公式：$x_{new} = x_{old} - \alpha \frac{d f}{dx}$
   - $\alpha$ 是学习率（步长）
   - 当导数为正，参数减小
   - 当导数为负，参数增大

2. **直观理解**
   ```
   在山坡上下山的类比：
   - 坡度（导数）为正 → 向左走（减小x）会下降
   - 坡度为负 → 向右走（增加x）会下降
   - 坡度为零 → 到达局部最低点
   ```

## 收敛过程

1. **逐步接近最小值**
   - 每次更新都朝着下降最快的方向移动
   - 随着接近最小值，导数逐渐变小
   - 最终在最小值处导数接近零

2. **学习率的作用**
   - 过大：可能跳过最小值（震荡或发散）
   - 过小：收敛太慢
   - 合适：稳定地接近最小值

## 常见变体

1. **[[随机梯度下降]]**
   - 每次使用单个样本计算梯度
   - 更新更频繁，但更嘈杂

2. **[[小批量梯度下降]]**
   - 每次使用一小批样本
   - 平衡了计算效率和更新稳定性

3. **[[自适应学习率]]方法**
   - [[Adam]]：自动调整每个参数的学习率
   - [[RMSprop]]：基于梯度平方的移动平均

## 实际应用注意事项

1. **局部最小值问题**
   - 梯度下降可能陷入局部最小值
   - 可以使用动量或随机初始化缓解

2. **梯度消失/爆炸**
   - 在[[深度神经网络]]中常见
   - 需要合适的[[激活函数]]和[[参数初始化]]

3. **收敛判断**
   - 梯度范数小于阈值
   - 损失函数变化小于阈值
   - 达到最大迭代次数

## 优化技巧

1. **动态学习率**
   - 开始时较大学习率快速接近
   - 接近最小值时减小学习率

2. **正则化**
   - 添加[[正则化项]]防止过拟合
   - 修改目标函数的形状

3. **批标准化**
   - 使用[[BatchNormalization]]改善梯度流动
   - 加速训练过程

#机器学习 #优化 #数学 #深度学习 